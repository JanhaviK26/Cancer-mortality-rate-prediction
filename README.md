Linear-Regression-and-Neural-Network-Regression
This project focuses on predicting cancer mortality rates using both linear regression and deep neural network (DNN) regression models. The dataset contains 3,047 samples with 33 features each, though some features have missing values, such as data on heavy alcohol consumption and regional pollution levels. The target variable is Target_death_rate. The dataset was split into 64% training, 16% validation, and 20% testing sets. Preprocessing involved removing features with excessive null values, dropping non-numeric and weakly correlated features, and normalizing the remaining features. For linear regression, log transformation was applied to reduce skewness, but this was not applied for DNN models.

Several models were trained, including linear regression and multiple DNN architectures (DNN-16, DNN-30-8, DNN-30-16-8, DNN-30-16-8-4). Linear regression achieved an R² of 0.68022, indicating reasonable performance with low variance but some bias due to its linear assumptions. The simpler DNN-16 model underfitted the data (R² ≈ 0.5107), showing high bias and low variance. Deeper models, such as DNN-30-16-8 and DNN-30-16-8-4, progressively reduced bias, improved R² scores to 0.5296–0.5303, but had higher variance, making them more prone to overfitting.

All models used the mean squared error (MSE) loss function, as it performed better than mean absolute error (MAE). The Adam optimizer was selected after attempts with SGD, which produced poor results. Learning rate experiments revealed that very high rates caused overshooting and poor convergence, while very low rates led to slow or stalled learning. Optimal learning rates were 0.01 for DNN-16 and 0.001 for deeper DNN models, balancing convergence speed and accuracy.

Plots of model performance across different learning rates confirmed these findings, showing that deeper DNN architectures could capture more complex patterns in the data, while simpler networks suffered from underfitting. In conclusion, this study demonstrates the importance of careful preprocessing, architecture selection, optimizer c
